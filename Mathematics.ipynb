{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Math behind the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Latent Semantic Relations and context word vectors using LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `D` be all the documents in the text corpus. (m number of documents $\\therefore D_{1\\times m}$) $\\\\$ \n",
    "Let `V` be the vocabulary of all words in it. (n number of words $\\therefore V_{1\\times n}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a term-document matrix $X_{n\\times m}$ where $\\\\$\n",
    "$\\large X_{i,j} = tf(v_i, d_j) \\times idf(v_i)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define: $\\\\\\large tf(v_i, d_j) = log(1+\\frac{f_{t_i,d_j}}{\\sum_{t^\\prime \\in V}f_{t^\\prime, d_j}})\\\\$\n",
    "$\\large idf(v_i) = log(\\frac{|D|}{|{d \\in D : t \\in d}|})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term-frequency($tf$) function calculates the logarithmically scaled frequency of the word $v_i$ ocuurring in the document $d_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse-document-frequency($idf$) function calculates the inverse of the probability that the word $v_i$ occurs in the word whole document `D`. This is done to negate the effect of words that are present in a huge number of documents as they hold lesser relevance.\n",
    "\n",
    "$\\large P(v/D) = \\frac {|{d \\in D \\ : \\ t \\in d}|}{|D|} \\\\$\n",
    "$\\large idf = -log(P(v/D)) = log(\\frac {1}{P(v/D)}) = log(\\frac{|D|}{|{d \\in D : t \\in d}|})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `n` is very large, it is difficult to gather information from this matrix X. Thus we perform Truncated Singular Value Decomposition on this matrix to get the most relevant information from the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is a complex mathematical procedure of factorizing a matrix into three factor matrices.\n",
    "\n",
    "$\\large X = U\\Sigma V^T$\n",
    "\n",
    "Here $U, V^T$ are orthogonal square matrices.\n",
    "$U$ is called the left singular matrix and $V^T$ is called the right singular matrix.\n",
    "$\\Sigma$ is the matrix of singular values at diagonal entries.\n",
    "\n",
    "$X = \\begin{bmatrix} x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\dots & \\vdots\\\\ x_{n1} & x_{n2} & \\dots & x_{nm}  \\end{bmatrix} = \n",
    "\\begin{bmatrix}  u_{11} & u_{12} & \\dots & \\dots & \\dots & u_{1n} \\\\  u_{21} & u_{22} & \\dots & \\dots & \\dots & u_{2n} \\\\ \\vdots & \\vdots & \\dots & \\dots & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\dots & \\dots &\\dots & \\vdots\\\\ \\vdots & \\vdots & \\dots & \\dots &\\dots & \\vdots\\\\ u_{n1} & u_{n2} & \\dots & \\dots & \\dots & u_{nn} \\end{bmatrix}\n",
    " . \\begin{bmatrix} \\sigma_1 & 0 & \\dots & 0  \\\\ 0 & \\sigma_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\dots & \\sigma_m \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 0 & \\dots & \\dots & 0 \\end{bmatrix} .\n",
    "  \\begin{bmatrix}  v_{11} & v_{12} & \\dots & v_{1m} \\\\  v_{21} & v_{22} & \\dots & v_{2m} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\dots & \\vdots\\\\ v_{m1} & v_{m2} & \\dots & v_{mm} \\end{bmatrix}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value matrix $\\Sigma$ has n singular values. These singular values are in descending order. Thus if we take the first `k` singular values only, it must approximate $X$ relatively good.\n",
    "\n",
    "$\\large \\therefore X^\\prime = U^\\prime \\Sigma^\\prime V^{T^\\prime}$ where $\\large U^\\prime, \\Sigma^\\prime \\ and \\ V^{T^\\prime}$ are truncated matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large U_{n\\times k}^\\prime$ is the the matrix of n words having k dimensional vectors.\n",
    "\n",
    "$\\large V_{k\\times m}^{T^\\prime}$ is the matrix of k semantic relations distributed across m documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Topics from $V^{T^\\prime}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\large K_i$ be the topic that represents the document $d_i$\n",
    "\n",
    "Then, $\\large K_i = argmax(V^{\\prime}_i)$\n",
    "\n",
    "The `argmax()` function returns the maximum entry from the $i^{th}$ column of $V^{T^\\prime}$ or $i^{th}$ row from $V^{\\prime}$\n",
    "\n",
    "Thus, now we have $K_{1\\times m}$ matrix of topics that represent the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Semantic Relations, Word Analogies and Word Meaning computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have calculated the latent semantic relations that are related through topics, we must now try to calculate the direct semantic relations between words itself. This will give us similarity between words and also word-sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we intend to do is to use 2 k-dimensional word-vectors per word and modify them such that we can predict context words given the center word. This will be done through skimming the vocabulary in windows of certain width and calculating the likelihood of a word being in the context of the given center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $v_w$ be the vector when `w` is the center word.\n",
    "\n",
    "Let $u_w$ be the vector when `w` is the context word.\n",
    "\n",
    "Since, we already have k dimensional vectors for our words in $U^\\prime$, we can use them as context vectors.\n",
    "\n",
    "And thus define randomly initialized k dimensional vectors for center word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 2 vectors so as to ease our further calculations as the self inner space product might cause complicated calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define, $\\large L(\\theta) = \\large {\\Pi_{t=1}^{T} \\Pi_{-m\\leq j\\leq m, j\\neq 0} P(w_{t+j}/w_t \\ ; \\ \\theta)}$\n",
    "\n",
    "where `t` is the position of the word in the corpus, `m` is the window size and $\\theta$ is the variable that needs to be optimized for better results.\n",
    "\n",
    "$\\theta = \\begin{bmatrix} v_{w_1} \\\\ v_{w_2} \\\\ \\vdots \\\\ v_{w_n} \\\\ u_{w_1} \\\\ \\vdots \\\\ u_{w_n} \\end{bmatrix}$ is the parameter for training the model.\n",
    "\n",
    "By Total Probability Theorem:\n",
    "\n",
    "$\\large P(o/c) = \\Large {\\frac {e^{u_o v_c}} {\\sum_{w\\in V} e^{u_w v_c}}}$, where `o` is the context word and `c` is the center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `softmax` function has been used to keep the probability positive and within the [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for the following model is defined as:\n",
    "\n",
    "$\\large J(\\theta) = -\\frac{1}{T} log(L(\\theta))$\n",
    "\n",
    "The negative sign is for us to be able to minimize the cost function as we need to increase the likelihood. And division by T is to get the average out total likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because:\n",
    "\n",
    "$\\large \\frac {\\partial}{\\partial v_c} log({\\frac {e^{u_o v_c}} {\\sum_{w\\in V} e^{u_w v_c}}}) = \\frac {\\partial}{\\partial v_c} log(e^{u_o v_c}) - \\frac {\\partial}{\\partial v_c} log(\\sum_{w\\in V} e^{u_w v_c})$\n",
    "\n",
    "$\\large = u_o - \\frac {\\partial}{\\partial v_c} log(\\sum_{w\\in V} e^{u_w v_c})$\n",
    "\n",
    "$\\large = u_o - \\frac {1}{\\sum_{w\\in V} e^{u_w v_c}} . \\frac {\\partial}{\\partial v_c}\\sum_{w\\in V} e^{u_w v_c}$\n",
    "\n",
    "$\\large = u_o - \\frac {\\sum_{w\\in V} u_w\\times e^{u_w v_c}}{\\sum_{w\\in V} e^{u_w v_c}} $\n",
    "\n",
    "Taking w = x in numerator\n",
    "\n",
    "$\\large = u_o - u_x.\\sum_{x\\in V}(\\frac {e^{u_x v_c}}{\\sum_{w\\in V} e^{u_w v_c}}) $\n",
    "\n",
    "$\\large = u_o - u_x.\\sum_{x\\in V}P(x/c) $\n",
    "\n",
    "= observed_vector - expected_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing the cost-function $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is an algorithm to minimize $J(\\theta)$ by changing $\\theta$\n",
    "\n",
    "=>Calculate Gradient of $J(\\theta)$\n",
    "\n",
    "=>Take small step in the direction of negative gradient\n",
    "\n",
    "=>Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\theta_{new} = \\theta_{old} - \\alpha \\nabla J(\\theta)$\n",
    "\n",
    "where $\\alpha$ is the step size and is a hyperparameter that can be varied to get appropriate result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for very large corpus $\\nabla J(\\theta)$ is very expensive to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we use Stochastic Gradient Descent to repeatedly sample windows and update after each small batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the softmax probability the normalizing term $\\large \\sum_{w\\in V} e^{u_wv_c}$ is expensive to compute, we can also use negative sampling.\n",
    "\n",
    "$\\large J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T J_t(\\theta)$\n",
    "\n",
    "where, $\\large J_t(\\theta) = log(\\sigma(u_ov_c)) + \\sum_{j \\sim P(w)}log(\\sigma(-u_jv_c))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\large \\sigma(x) = \\frac {1}{1+e^{-x}}$ is the sigmoid function that keeps the value between [0,1]\n",
    "\n",
    "We sample the random words based on their frequency occurrence. $P(w) = {U(w)^{\\frac{3}{4}}}$, where $U(w)$ is the unigram distribution of the whole corpus, in which words are distributed as per their ocurrences in the corpus $\\therefore U(w) = f_{w,D}$. The 3/4 power is to make the words with low ocurrences also get sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term with $\\sigma (u_ov_c)$ is to maximize the probability when two words co-occur. While the second term with the negative sign $\\sigma (-u_jv_c)$ is to minimize the probability for the noise words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaning Component in Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the stochastic gradient descent, we get the improved word vectors in $\\theta$\n",
    "\n",
    "To analyze meanings from these word vectors we can use cosine similarity to calculate similar words with similar usages/meanings.\n",
    "\n",
    "$\\large cos(\\theta) = \\frac {v_1.v_2}{||v_1||.||v_2||}$\n",
    "\n",
    "$\\large \\therefore w_m = argmax_i(\\frac {v_w.v_{w_i}}{||v_w||.||v_{w_i}||})$, where $v_w$ is the vector for the word for which the meaning component is to be calculated and $v_{w_i}$ are the vector for the words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Analogies Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Analogies can be computed using the meaning component of the word vectors.\n",
    "\n",
    "Let $w_a:w_b::w_c:(w_d)?$, then $\\large w_d = argmax_i \\frac{((v_{w_b}-v_{w_a}+v_{w_c}).v_{w_i})}{|v_{w_b}-v_{w_a}+v_{w_c}|.|v_{w_i}|}$\n",
    "\n",
    "Here we are doing vector addition. Taking an example, if man : king :: woman : ?. Then we remove the vector of man fromm the vector of king and add the vector of woman to it.($v_{king} - v_{man} + v_{woman}$) and check which word is closest to meaning to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
